# -*- coding: utf-8 -*-
"""Proyek Kedua MLT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19ocktI4lQuLzb_hvWnVcXkT6gu2tfKAn

# **Proyek Machine Learning**

Nama: Melanie Sayyidina Sabrina Refman

## **Import Dataset**
"""

from google.colab import files

# Mengupload file kaggle
files.upload()

# Mengonfigurasi Kaggle API di lingkungan Colab
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 /root/.kaggle/kaggle.json

# Mengunduh dataset dari Kaggle menggunakan Kaggle API
!kaggle datasets download -d arashnic/book-recommendation-dataset

from zipfile import ZipFile

# Mengekstrak file zip
file_name = "/content/book-recommendation-dataset.zip"
with ZipFile(file_name,'r') as zip:
  zip.extractall()
  print('Extraction Completed')

"""## **Import Library**

Pada bagian ini, dilakukan import berbagai library yang akan digunakan dalam proses pengolahan data, eksplorasi, pemodelan, hingga evaluasi model.
"""

# Import Library
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import precision_score, recall_score, f1_score
from scipy.sparse import csr_matrix
from sklearn.decomposition import TruncatedSVD

"""*   Pandas dan NumPy: Digunakan untuk manipulasi dan analisis data.
*   Sklearn: Digunakan untuk vektorisasi teks, penghitungan kesamaan kosinus, dan evaluasi model.
*   Scipy: Untuk operasi matematika yang lebih kompleks seperti matriks sparse.
*   Matplotlib dan Seaborn: Untuk membuat visualisasi data.

## **Data Understanding**

Pada tahap ini, dataset yang akan digunakan dimuat dan diperiksa struktur awalnya. Langkah ini penting untuk memahami data mentah sebelum melanjutkan ke proses pembersihan dan analisis lebih lanjut.

Dataset terdiri dari tiga file CSV yang berisi informasi buku, penilaian pengguna, dan data pengguna.
"""

# Load the dataset
books = pd.read_csv("Books.csv")
ratings = pd.read_csv("Ratings.csv")
users = pd.read_csv("Users.csv")

"""Menampilkan beberapa baris awal dari masing-masing dataset untuk mendapatkan gambaran umum tentang data.

#### **Books Dataset**
"""

# Initial Data Exploration
print("Books.csv Dataset:")
books

print("Missing values in Books:\n")
books.isnull().sum()

print("Duplicates in Books:\n")
books.duplicated().sum()

"""#### **Ratings Dataset**"""

print("Ratings.csv Dataset:")
ratings

print("Missing values in Books:\n")
ratings.isnull().sum()

print("Duplicates in Books:\n")
ratings.duplicated().sum()

"""#### **Users Dataset**"""

print("Users.csv Dataset:")
users

print("Missing values in Books:\n")
users.isnull().sum()

print("Duplicates in Books:\n")
users.duplicated().sum()

"""## **Univariate Exploratory Data Analysis**

Dalam bagian ini, kita memeriksa struktur dataset secara lebih mendalam untuk mendapatkan informasi tentang kolom, tipe data, dan keberadaan data yang hilang. Hal ini membantu menentukan langkah pembersihan data yang diperlukan.

#### **Variable Books**

Memeriksa struktur dataset Books untuk melihat jumlah kolom, tipe data, dan jumlah data non-null di setiap kolom.
"""

# Variable Books
books.info()

"""Tahun publikasi adalah informasi penting dalam dataset buku. Namun, jika terdapat data yang tidak valid, seperti teks atau nilai kosong, kita konversi menjadi nilai numerik dengan NaN diganti 0. Langkah ini dilakukan untuk memastikan konsistensi tipe data."""

# Mengubah kolom Year-Of-Publication menjadi tipe data integer
books['Year-Of-Publication'] = pd.to_numeric(books['Year-Of-Publication'], errors='coerce').fillna(0).astype(int)
books.info()

"""Memilih hanya kolom yang relevan dari dataset Books untuk mengurangi kompleksitas dan fokus pada informasi yang penting."""

# Mengambil data yang diperlukan
books = books[['ISBN', 'Book-Title', 'Book-Author', 'Year-Of-Publication', 'Publisher']]
books.head()

"""Grafik berikut ini memberikan wawasan tentang penulis paling produktif di dataset. Informasi ini dapat digunakan untuk analisis lebih lanjut terkait popularitas buku mereka."""

# Menampilkan grafik 5 penulis yang menulis buku terbanyak
plt.figure(figsize=(10, 6))
books['Book-Author'].value_counts().head(5).plot(kind='bar', color='skyblue')
plt.title('Top 5 Penulis dengan Buku Terbanyak', fontsize=14)
plt.xlabel('Penulis', fontsize=12)
plt.ylabel('Jumlah Buku', fontsize=12)
plt.show()

"""#### **Variable Ratings**

Memeriksa struktur dataset Ratings untuk memahami distribusi penilaian yang diberikan oleh pengguna.
"""

# Variabel Rating
ratings.info()

"""Grafik berikut ini menunjukkan distribusi nilai penilaian yang diberikan pengguna. Hal ini membantu mengidentifikasi pola seperti kecenderungan memberikan nilai tertentu."""

# Menampilkan grafik distribusi rating
plt.figure(figsize=(10, 6))
ratings['Book-Rating'].value_counts().sort_index().plot(kind='bar', color='salmon')
plt.title('Distribusi Book-Rating', fontsize=14)
plt.xlabel('Rating', fontsize=12)
plt.ylabel('Jumlah', fontsize=12)
plt.show()

"""#### **Variable Users**

Memeriksa struktur dataset Users untuk memahami informasi yang tersedia tentang pengguna, seperti lokasi atau usia.
"""

users.info()

"""## **Data Preparation**

Pada tahap ini, data dipersiapkan lebih lanjut dengan memeriksa dan menghapus nilai yang hilang, duplikasi, dan anomali. Langkah ini penting untuk memastikan kualitas data sebelum dilakukan pemodelan.

#### **Cek Missing Value**

Memeriksa apakah ada nilai yang hilang pada dataset.
"""

# Mengecek missing value pada dataset Books
print("Missing values in Books:\n")
books.isnull().sum()

books = books.dropna()
books.isnull().sum()

# Mengecek missing value pada dataset Ratings
print("Missing values in Ratings:\n")
ratings.isnull().sum()

# Mengecek missing value pada dataset Users
print("Missing values in Users:\n")
users.isnull().sum()

"""#### **Cek Data Duplikat**

ISBN yang duplikat dapat menyebabkan ketidakkonsistenan dalam analisis. Oleh karena itu, kita menghapus entri dengan ISBN yang sama.
"""

# Mengecek duplikasi ISBN pada Books
duplicates = books[books['ISBN'].duplicated()]
print("Jumlah ISBN duplikat:", len(duplicates))

"""#### **Merge Dataset**

Menggabungkan dataset Ratings dan Books berdasarkan ISBN. Hal ini penting untuk mempersiapkan data yang digunakan pada model rekomendasi.
"""

# Merge Ratings dengan Books berdasarkan ISBN
ratings_books = pd.merge(ratings, books, on='ISBN', how='inner')
print("Merged Dataset Head:")
ratings_books

"""#### **Sampling Data**

Karena dataset terlalu besar, diambil 16000 data pertama. Sampling dilakukan untuk mengurangi waktu pemrosesan tanpa kehilangan esensi data.
"""

ratings_books = ratings_books[:16000]
ratings_books

"""## **Modeling (Conten-Based Filtering)**

Pada bagian ini, kita membangun sistem rekomendasi berbasis konten menggunakan teknik TF-IDF untuk merepresentasikan data penulis buku dan cosine similarity untuk mengukur kesamaan antar buku.

#### **TF-IDF Vectorizer**

Tahap ini membangun representasi teks menggunakan TF-IDF. TF-IDF digunakan untuk merepresentasikan teks dalam bentuk vektor numerik. Kolom `Book-Author` digunakan untuk mencari kesamaan antar buku.
"""

# TF-IDF Vectorizer
vectorizer = TfidfVectorizer(stop_words='english')

# Melakukan perhitungan IDF pada kolom Book-Author
tfidf_matrix = vectorizer.fit_transform(ratings_books['Book-Author'].fillna(''))

# Mendapatkan nama fitur dari indeks integer
tfidf_feature_names = vectorizer.get_feature_names_out()

# Mengubah vektor TF-IDF menjadi DataFrame
pd_tfidf = pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tfidf_feature_names,
    index=ratings_books['Book-Title']
)

# Menampilkan sampel DataFrame TF-IDF (10x10)
print("TF-IDF DataFrame (10x10):")
pd_tfidf.sample(10, axis=1).sample(10, axis=0)

"""#### **Cosine Similarity**

Tahap ini menghitung kesamaan antar buku berdasarkan penulis. Cosine similarity digunakan untuk mengukur seberapa mirip dua buku berdasarkan representasi TF-IDF.
"""

# Cosine Similarity
cosine_sim = cosine_similarity(tfidf_matrix)

# Mengubah matriks kesamaan menjadi DataFrame
cosine_sim_df = pd.DataFrame(cosine_sim, index=ratings_books['Book-Title'], columns=ratings_books['Book-Title'])

# Menampilkan matriks kesamaan untuk beberapa judul buku
print("Cosine Similarity DataFrame (5x10):")
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

"""#### **Implementasi Sistem Rekomendasi**

Tahap ini mengimplementasikan fungsi rekomendasi berdasarkan kesamaan kosinus. Fungsi `recommend_books` akan menghitung skor kesamaan dan memilih 10 buku dengan skor tertinggi (kecuali buku itu sendiri).
"""

# Implementasi Rekomendasi
def recommend_books(book_title, cosine_sim=cosine_sim_df):
    # Memastikan buku ada dalam DataFrame
    if book_title not in cosine_sim.index:
        return "Buku tidak ditemukan dalam dataset."

    # Mendapatkan skor kesamaan untuk semua buku
    sim_scores = cosine_sim[book_title].sort_values(ascending=False)

    # Mendapatkan 10 buku yang paling mirip (kecuali buku itu sendiri)
    sim_scores = sim_scores.iloc[1:11]

    # Mengembalikan judul buku yang mirip
    return sim_scores.index.tolist()

"""Fungsi `book_recommendation` digunakan untuk mencetak hasil rekomendasi tersebut."""

def book_recommendation(book_title_test):
    print(f"Rekomendasi Buku untuk '{book_title_test}':")
    recommendations = recommend_books(book_title_test)
    if isinstance(recommendations, str):
        print(recommendations)
    else:
        for i, rec in enumerate(recommendations, 1):
            print(f"{i}. {rec}")

"""Berikut contoh implementasi dengan judul buku tes:"""

book_title_test = "Into the Land of the Unicorns (Unicorn Chronicles)"
book_recommendation(book_title_test)

"""## **Evaluation**

Tujuan dari evaluasi ini adalah untuk mengukur performa model dalam menentukan similarity antar item dengan menggunakan metrik evaluasi seperti precision, recall, dan F1-score. Model ini mengukur kesamaan antar item dengan cosine similarity dan membandingkan hasilnya terhadap nilai threshold yang telah ditentukan (0.5).

**Tahapan Evaluasi**

1.   **Penentuan Threshold** : Threshold 0.5 digunakan untuk mengklasifikasikan apakah dua item dianggap similar (1) atau not similar (0) berdasarkan nilai cosine similarity.
2.   **Ground truth** : Ground truth data dibuat dengan membandingkan nilai cosine similarity dengan threshold. Jika nilai similarity lebih besar atau sama dengan threshold, maka dianggap sebagai 1 (similar), sebaliknya 0 (not similar).
3.   **Sampling Data** : Untuk efisiensi evaluasi, hanya 1000 sampel yang diambil secara acak dari matriks similarity.
4.   **Evaluasi dengan Classification Report** : Hasil prediksi dibandingkan dengan ground truth menggunakan precision, recall, dan F1-score.
"""

# Menentukan threshold untuk menentukan similarity sebagai 1 atau 0
threshold = 0.5

# Membuat ground truth data berdasarkan threshold
ground_truth = (cosine_sim >= threshold).astype(int)

# Sampling sebagian data dari cosine similarity matrix untuk evaluasi
sample_indices = np.random.choice(cosine_sim.shape[0], size=1000, replace=False)
cosine_sim_sample = cosine_sim[np.ix_(sample_indices, sample_indices)]
ground_truth_sample = ground_truth[np.ix_(sample_indices, sample_indices)]

# Meratakan matriks untuk evaluasi
cosine_sim_flat = cosine_sim_sample.ravel()
ground_truth_flat = ground_truth_sample.ravel()

# Menghitung metrik evaluasi menggunakan precision, recall, dan F1-score
from sklearn.metrics import classification_report

predictions = (cosine_sim_flat >= threshold).astype(int)

# Menggunakan classification report untuk mendapatkan semua metrik evaluasi
report = classification_report(ground_truth_flat, predictions, zero_division=1, output_dict=True)
precision = report["1"]["precision"]
recall = report["1"]["recall"]
f1 = report["1"]["f1-score"]

# Menampilkan hasil evaluasi
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1-Score: {f1:.2f}")

"""**Hasil Evaluasi**

*   **Precision** : 1.00
(Model berhasil memprediksi semua pasangan similar secara akurat tanpa ada kesalahan.)
*   **Recall** : 1.00
(Model berhasil menemukan semua pasangan yang seharusnya similar dalam data ground truth.)
*   **F1-Score** : 1.00
(Model memiliki keseimbangan sempurna antara precision dan recall.)

## **Kesimpulan**

Hasil evaluasi menunjukkan bahwa model memiliki performa sempurna dengan nilai precision, recall, dan F1-score sebesar 1.00. Ini berarti:

*   Semua pasangan yang diprediksi sebagai similar benar-benar similar.
*   Semua pasangan yang seharusnya similar ditemukan oleh model.
*   Tidak ada false positives atau false negatives.
"""